
# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `3부. 데이터 분석하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_7th_TIL

### 3부. 데이터 분석하기
### 13.머신러닝 분석 방법론
### 14.모델 평가



## Study Schedule

|주차 | 공부 범위     | 완료 여부 |
|----|----------------|----------|
|1주차| 1부 p.2~56     | ✅      |
|2주차| 1부 p.57~79    | ✅      | 
|3주차| 2부 p.82~120   | ✅      | 
|4주차| 2부 p.121~202  | ✅      | 
|5주차| 2부 p.203~254  | ✅      | 
|6주차| 3부 p.300~356  | ✅      | 
|7주차| 3부 p.357~615  | ✅      | 

<!-- 여기까진 그대로 둬 주세요-->

# 13.머신러닝 분석 방법론

```
✅ 학습 목표 :
* 선형 회귀와 다항 회귀를 비교하고, 데이터를 활용하여 적절한 회귀 모델을 구축할 수 있다. 
* 로지스틱 회귀 분석의 개념과 오즈(Odds)의 의미를 설명하고, 분류 문제에 적용할 수 있다.
* k-means 알고리즘의 원리를 설명하고, 적절한 군집 개수를 결정하여 데이터를 군집화할 수 있다.
```

## 13.1. 선형 회귀분석과 Elastic Net(예측모델)
회귀분석은 종속변수 \\( y \\)가 하나 혹은 다수의 독립변수 \\( x_1, x_2, \\ldots, x_p \\)에 의해 어떻게 영향을 받는지를 분석하는 통계 기법이다. 이때 모델은 다음과 같이 표현된다.

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon
$$

- \\( \beta_0 \\): 절편 (Intercept)  
- \\( \beta_i \\): 각 독립변수의 회귀 계수  
- \\( \varepsilon \\): 오차항 (예측 불가능한 요소)

**기원**  
프랜시스 골턴이 부모와 자식의 키 간 관계를 분석하며 회귀 개념 도입.

**분류**  
- 단순 선형 회귀: 독립변수가 하나  
- 다중 선형 회귀: 독립변수가 여러 개 → 다중공선성(multicollinearity) 존재 여부 확인 필요 (VIF, Tolerance 등)

---

>### 13.1.1 다항 회귀 (Polynomial Regression)

비선형적 관계를 설명하기 위해 독립변수의 고차항을 포함시킨 모델.

$$
y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \varepsilon
$$

- 차수가 증가하면 유연성이 향상되지만 과적합 위험도 증가  
- 기본 가설 검정:  
  - 귀무가설: 모든 회귀 계수 = 0  
  - 대립가설: 적어도 하나의 회귀 계수 ≠ 0

---

>### 13.1.2 변수 선택 기법

변수가 많은 상황에서 불필요한 변수 제거를 통해 모델 간소화 및 성능 향상.

1. **전진 선택법 (Forward Selection)**  
   절편만 있는 모델에서 출발해 유의미한 변수를 하나씩 추가. 한 번 포함된 변수는 제거되지 않음.

2. **후진 제거법 (Backward Elimination)**  
   모든 변수를 포함한 상태에서 출발. 유의하지 않은 변수부터 제거.

3. **단계적 선택법 (Stepwise Selection)**  
   전진·후진 방식 혼합. 선택된 변수가 3개 이상이면 추가·제거를 반복하며 최적 조합 탐색.

---


>### 13.1.3 Ridge, Lasso 그리고 Elastic Net

선형 회귀에서 과적합을 방지하고 모델의 일반화 성능을 높이기 위해 **계수 정규화(regularization)** 기법이 사용된다. 대표적인 방법은 **Ridge**, **Lasso**, 그리고 이 둘을 결합한 **Elastic Net**이다.

#### Ridge 회귀 (L2 정규화)

- Ridge는 **모든 변수의 계수를 유지**하면서 그 크기를 조정한다.
- 불필요한 변수의 계수는 0에 가까운 값으로 축소됨
- 정규화 항으로 L2-norm (제곱합)을 사용

$$
\text{Loss}_{Ridge} = RSS + \alpha \sum_{j=1}^p \beta_j^2
$$

- alpha : 정규화 강도 (0이면 OLS와 동일)
- 다중공선성을 완화하면서 모델 설명력을 보존할 수 있음

#### Lasso 회귀 (L1 정규화)

- Lasso는 **중요한 일부 변수만 선택**하고 나머지 변수의 계수를 0으로 만들어 제거
- 변수 선택이 내장되어 있어 모델을 **단순화**하는 데 유리
- 정규화 항으로 L1-norm (절댓값합)을 사용

$$
\text{Loss}_{Lasso} = RSS + \alpha \sum_{j=1}^p |\beta_j|
$$

- alpha 값에 따라 불필요한 변수 제거 가능
- 해석이 쉽고, 희소 모델(sparse model)을 생성할 수 있음

#### Elastic Net

- Ridge와 Lasso의 **혼합 형태**로 두 정규화 항을 모두 포함

$$
\text{Loss}_{EN} = RSS + \lambda \left[ (1 - r) \sum_{j=1}^p \beta_j^2 + r \sum_{j=1}^p |\beta_j| \right]
$$

- r : 혼합 비율 (0에 가까우면 Ridge, 1에 가까우면 Lasso처럼 작동)
- Ridge는 계수를 0으로 만들 수 없지만, Lasso는 0으로 가능함
- 변수 선택이 중요한 경우 Lasso 비중 ↑, 모든 변수를 포함하고자 한다면 Ridge 비중 ↑


>### 13.1.4 회귀 분석 결과 해석

다중 회귀 분석 결과는 다음 요소들을 통해 해석할 수 있다.

### 핵심 해석 지점

1. **P-Value**  
   - 0.05보다 작으면 통계적으로 유의함
   - 적어도 하나 이상의 독립변수가 종속변수에 의미 있는 영향을 미친다고 판단 가능

2. **R-Square (결정계수)**  
   - 독립변수가 종속변수를 얼마나 잘 설명하는지를 나타냄
   - 예: R² = 0.773 → 전체 변동의 77.3%를 설명함

3. **Adjusted R-Square (수정 결정계수)**  
   - 변수의 수가 많을수록 R²이 과대 추정되는 문제를 보정
   - 다중회귀에서는 **Adj R²을 비교 기준**으로 삼는 것이 더 타당

4. **Root MSE**  
   - 예측값과 실측값 간 오차의 제곱평균의 제곱근 (예: RMSE = 11.51)

**예시 해석 요약**

| 지표 | 해석 |
|------|------|
| **P-value < 0.05** | 유의한 독립변수가 존재함 |
| **R-Square = 0.773** | 77.3%의 설명력 |
| **Adj R-Square = 0.745** | 변수 수 고려한 보정 설명력 |
| **Root MSE = 11.51** | 평균 예측 오차 약 11.51 |

---

**P-value vs R-Square 조합에 따른 모델 해석 전략**

| P-value | R-Square | 전략 |
|---------|----------|-------|
| 낮음 (<0.05) | 높음 | 이상적인 모델, 주요 인자 추출 |
| 낮음 (<0.05) | 낮음 | 주요 인자는 있으나 설명력 부족 → 변수 추가 / 비선형 회귀 |
| 높음 (>0.05) | 높음 | 이상치 제거 / 데이터 확보 필요 |
| 높음 (>0.05) | 낮음 | 새 변수 탐색 / 모델 재설계 |

> 해석 시 항상 P-value와 R²을 **동시에 고려**해야 하며, R²만 보고 모델 적합도를 평가하는 것은 위험할 수 있다.


## 13.2. 로지스틱 회귀분석 (분류모델)

범주형 종속변수의 분류에 사용되는 회귀모델. 확률을 예측하며, 출력은 0~1 사이의 값.

**기본 로짓 함수**

$$
\log \left( \frac{p}{1 - p} \right) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p
$$

오즈(odds): 사건 발생 가능성 / 비발생 가능성  
오즈에 로그를 취한 값이 로짓(logit)

**최종 식**

$$
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p)}}
$$

**다항 로지스틱 회귀**  
종속변수 범주가 3개 이상인 경우 K - 1개의 식이 필요

## 13.8. k-means 클러스터링(군집모델)
**K-Means**는 **비지도 학습(Unsupervised Learning)** 방식으로, 사전 레이블 없이 관측치 간의 유사성을 기준으로 데이터를 군집화한다.

**핵심 개념**
- `K`는 군집의 개수, `Means`는 각 군집의 중심값(centroid)을 의미
- 관측치 간의 거리(주로 유클리디안 거리)를 기준으로 분류하며, 군집 내 제곱거리합(WCSS, Within-Cluster Sum of Squares)을 최소화하는 방향으로 수렴

**알고리즘 절차**
1. \( k \)개의 중심점을 무작위로 초기화
2. 각 데이터 포인트를 가장 가까운 중심점에 할당
3. 각 군집의 중심점을 다시 계산 (군집 내 평균값)
4. 중심점이 더 이상 변하지 않을 때까지 2~3단계를 반복

**목적 함수 (최적화 대상)**

$$
\underset{C}{\text{argmin}} \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2
$$

- Ci: i번째 군집

**적절한 k값 선택 방법**
- **도메인 지식**: 문제 맥락에 따라 적절한 k 선정
- **엘보우 기법(Elbow Method)**: WCSS 값이 급감하다가 완만해지는 지점의 k 선택
- **실루엣 계수(Silhouette Score)**: 군집 내 일관성과 군집 간 분리를 동시에 고려


# 14. 모델 평가

```
✅ 학습 목표 :
* 유의확률(p-value)을 해석할 때 주의할 점을 설명할 수 있다.
* 분석가가 올바른 주관적 판단을 위한 필수 요소를 식별할 수 있다.
```

## 14.3. 회귀성능 평가지표
**결정계수 (R-Square)**  
종속변수의 변동을 독립변수로 얼마나 설명할 수 있는지를 나타내는 지표:

$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$

- \( SS_{res} \): 잔차 제곱합 (Residual Sum of Squares)  
- \( SS_{tot} \): 총 제곱합 (Total Sum of Squares)

**수정 결정계수 (Adjusted R-Square)**  
독립변수 수가 많아지면서 과도하게 증가하는 R²을 보정한 지표:

$$
\bar{R}^2 = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - p - 1} \right)
$$

- \( n \): 관측치 수, \( p \): 독립변수 수

**RMSE (Root Mean Square Error)**

$$
RMSE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 }
$$

**MAE (Mean Absolute Error)**

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

**MAPE (Mean Absolute Percentage Error)**

$$
MAPE = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
$$

**RMSLE (Root Mean Squared Logarithmic Error)**

$$
RMSLE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} \left( \log(y_i + 1) - \log(\hat{y}_i + 1) \right)^2 }
$$


## 14.6. 유의확률의 함정
**p-value는 절대적인 진리도, 확률도 아니다.**

- \( p < 0.05 \)라고 해서 항상 의미 있는 관계가 있다고 해석해선 안 됨
- 표본 크기가 클수록 작은 차이도 유의하게 나타날 수 있음
- p값은 단지 귀무가설 하에서 현재 데이터가 관측될 확률일 뿐

**2016 미국 통계학회 ASA 성명 요약**
- p값은 단독으로 의사결정의 기준이 될 수 없다.
- 효과 크기, 신뢰구간, 도메인 지식, 실험 설계와 함께 해석해야 한다.
- p값이 낮더라도, 실제로 의미 있는 관계나 인과관계를 보장하지 않는다.

> **통계적 유의성과 실질적 중요성은 다르다.**


## 14.7. 분석가의 주관적 판단과 스토리텔링
모델이 수치를 출력하더라도, **그 의미를 해석하고 전략으로 연결하는 역할은 사람의 몫**이다.

**우수한 분석가가 갖춰야 할 3요소**

1. **도메인 지식**  
   - 해당 산업/현장의 맥락을 이해해야, 데이터의 의미와 이상치를 올바르게 해석할 수 있음

2. **탐색적 데이터 분석(EDA) 및 전처리 능력**  
   - 데이터 오류 수정, 변수 변환, 누락값 처리 등 실무에서 중요한 전처리 역량

3. **커뮤니케이션 능력과 스토리텔링**  
   - 숫자를 넘어 문제 → 분석 → 해석 → 전략으로 이어지는 **서사 구조**를 설계해야 함



<br>
<br>

# 확인 문제

## **문제 1. 선형 회귀**

> **🧚 칼 피어슨의 아버지와 아들의 키 연구 결과를 바탕으로, 다음 선형 회귀식을 해석하세요.**  
> 칼 피어슨(Karl Pearson)은 아버지(X)와 아들(Y)의 키를 조사한 결과를 바탕으로 아래와 같은 선형 회귀식을 도출하였습니다. 아래의 선형 회귀식을 보고 기울기의 의미를 설명하세요. 
>  
> **ŷ = 33.73 + 0.516X**  
>   
> - **X**: 아버지의 키 (cm)  
> - **ŷ**: 아들의 예상 키 (cm)  

```
이 회귀식에서 계수 0.516은 아버지의 키가 1cm 증가할 때 아들의 키는 평균적으로 0.516cm 증가함을 나타낸다. 
```
---

## **문제 2. 로지스틱 회귀**  

> **🧚 다트비에서는 학생의 학업 성취도를 예측하기 위해 다항 로지스틱 회귀 분석을 수행하였습니다. 학업 성취도(Y)는 ‘낮음’, ‘보통’, ‘높음’ 3가지 범주로 구분되며, 독립 변수는 주당 공부 시간(Study Hours)과 출석률(Attendance Rate)입니다. 단, 기준범주는 '낮음' 입니다.**   

| 변수 | Odds Ratio Estimates | 95% Wald Confidence Limits |  
|------|----------------------|--------------------------|  
| Study Hours | **2.34** | (1.89, 2.88) |  
| Attendance Rate | **3.87** | (2.92, 5.13) |  

> 🔍 Q1. Odds Ratio Estimates(오즈비, OR)의 의미를 해석하세요.

<!--변수 Study Hours의 오즈비 값이 2.34라는 것과 Attendance Rate의 오즈비 값이 3.87이라는 것이 각각 무엇을 의미하는지 구체적으로 생각해보세요.-->

```
- Study Hours의 오즈비 2.34는 주당 공부 시간이 1단위 증가할 때, 학업 성취도가 '낮음'에서 '보통' 또는 '높음'으로 분류될 가능성이 약 2.34배 높아진다는 것을 의미한다.

- Attendance Rate의 오즈비 3.87은 출석률이 1단위 증가할 때, 학업 성취도가 '낮음'에서 더 높은 수준으로 분류될 가능성이 약 3.87배 증가한다는 의미다.
```

> 🔍 Q2. 95% Wald Confidence Limits의 의미를 설명하세요.
<!--각 변수의 신뢰구간에 제시된 수치가 의미하는 바를 생각해보세요.-->

```
- Study Hours의 신뢰구간 (1.89, 2.88)은 오즈비가 95% 확률로 이 범위 내에 있을 것으로 추정된다는 뜻이다. 이 구간에 1이 포함되지 않으므로 공부 시간은 학업 성취도에 통계적으로 유의한 영향을 준다.

- Attendance Rate의 신뢰구간 (2.92, 5.13)도 마찬가지로, 출석률이 학업 성취도에 유의미한 영향을 미치는 변수임을 보여준다.
```

> 🔍 Q3. 이 분석을 기반으로 학업 성취도를 향상시키기 위한 전략을 제안하세요.
<!--Study Hours와 Attendance Rate 중 어느 변수가 학업 성취도에 더 큰 영향을 미치는지를 고려하여, 학업 성취도를 향상시키기 위한 효과적인 전략을 구체적으로 제시해보세요.-->

```
- 두 변수 중에서 출석률의 오즈비가 더 크기 때문에, 학업 성취도에 더 큰 영향을 미친다고 볼 수 있다. 따라서 출석률을 개선하는 것이 우선 전략이 될 수 있다.

- 예를 들어, 무단결석 감소를 위한 관리 체계 강화, 출석 인센티브 제공, 교사와의 피드백 강화 등을 통해 출석률을 높이는 것이 효과적이다.

```

---


## **문제 3. k-means 클러스터링**

> **🧚 선교는 고객을 유사한 그룹으로 분류하기 위해 k-means 클러스터링을 적용했습니다. 초기에는 3개의 군집으로 설정했지만, 결과가 만족스럽지 않았습니다. 선교가 최적의 군집 수를 찾기 위해 사용할 수 있는 방법을 한 가지 이상 제시하고 설명하세요.**

```
1. 엘보우 기법 (Elbow Method)  
   군집 수를 증가시키며 각 k에 대한 군집 내 제곱합(WCSS: Within-Cluster Sum of Squares)을 계산한다.  
   이 값은 군집 수가 많아질수록 감소하지만, 어느 시점부터는 감소 폭이 급격히 줄어든다.  
   이 지점이 '팔꿈치(elbow)'처럼 꺾이는 지점이며, 그 지점의 k를 최적 군집 수로 선택할 수 있다.

2. 실루엣 계수 (Silhouette Score)  
   각 데이터 포인트에 대해 같은 군집 내 데이터와의 거리 평균과, 가장 가까운 다른 군집과의 거리 평균을 비교해 군집화 적절성을 측정한다.  
   실루엣 계수는 -1에서 1 사이의 값을 가지며, 값이 클수록 잘 분리된 군집을 의미한다.  
   여러 k 값에 대해 실루엣 계수를 계산하고, 가장 높은 값을 가지는 k를 선택한다.

```

### 🎉 수고하셨습니다.
